Рассматриваем численные методы решения задачи Коши.
\[
 \begin{cases}
 y'(x) = f(x,y);\\
 y(x_0) = y_0.
\end{cases},\quad x\in [x_0,X].
\]

Мы рассмотрели метод Эйлера, где производная заменяется на простейшее разностное соотношение. Мы считали, что отрезок разбит сеткой. Не обязательно равномерной.
\[
  y_{n+1} = y_n  + h_n f_(x_n,y_n),\quad y_0.
\]

Пока будем считать, что $h_n = h$ не зависит от номера шага. У нас есть точность решения $E_n = y(x_n) - y_n$. Показали для метода Эйлера $E_n = O(h)$. Также рассмотрели глобальную погрешность $e_{n+1} = \Til y(x_{n+1}) - y_{n+1}$, где
\[
  \begin{cases}
\Til y'(x) = f(x,\Til y),\\
\Til y(x_n) = y_n.
\end{cases}
\]
Выяснили, что $e_{n+1} = O(h^2)$.

\subsection{Методы Рунге"--~Кутта}
Та же самая задача Коши у нас остаётся. Я могу сразу выписать формулу, но попробую обосновать.

Будем считать, что мы находимся в такой ситуации: у нас есть сетка. Имеем точку $x_n$, дотопали. Хотим сделать один шаг в $x_{n+1} = x_n + h$. Предполагается знание решение только в одной точке. Будем минимизировать, стало быть, локальную погрешность, то есть считать, что в $x_n$ значение функции мы знаем точно.
\[
  y_{n+1} = y(x_n) + \int\limits_{x_n}^{x_{n+1}} f\big(x,y(x)\big)\,dx.
\]
Ага, говорим, интеграл мы умеем численно интегрировать. Давайте на отрезке $[x_n,x_{n+1}]$ возьмём как-нибудь узлы: $x_n^{(1)} = x_n + \alpha_1 h$, $x_{n}^{(2)} = x_n + \alpha_2 h$, \dots $x_n^{(m)} = x_n + \alpha_m h$, где $0=\alpha_1 <\alpha_2<\dots<\alpha_m\le 1$. Построим квадратурную формулу. Способов много. Получим
\[
  y(x_{n+1}) \approx y(x_n) + h \RY i1m c_i f\big(x_n^{(i)},y(x_n^{(i)})\big).
\]
Мы по этой формуле считать, не можем. мы же не знаем $y$ в точках правее $x_n$. Давайте попробуем считать последовательно. Воспользуемся такой вот вещью 
\[
y(x_n^{(i)}) = y(x_n) + \int\limits_{x_n}^{x_n^{(i)}}\,dx.
\]

Скажем, что
\[
  y(x_n^{(2)})\approx y(x_n) + h \beta_{21} f\big(x_n^{(1)},y(x_n^{(1)})\big).
\]
Как-то потом подберём $\beta_{21}$. Ну то есть мы здесь по одному узлу посчитали и успокоились. Что делать дальше-то?
\[
  y(x_{n}^{(3)}) \approx y(x_n) + h \beta_{31} f\big(x_n^{(1)}, y(x_n^{(1)})\big) + h \beta_{32} f\big(x_n^{(2)},y(x_n^{(2)})\big).
\]
Ну и так далее, формула будет разрастаться.

Мы за погрешностью на этом этапе не пытались следить. Просто абы как выбрали формулы. А теперь давайте всё вместе объединим и в совокупности уже будем оценивать погрешности и подбирать лучшие коэффициенты из всей совокупности.

Фиксируем целое положительное число $m$. Затем по этому числу $m$ мы фиксируем три набора параметров $p_1,\dots,p_m$, $\alpha_2,\dots,\alpha_m$, $\beta_{ij},\ 0<j<i\le m$. Далее пишем
\begin{eqnarray*}
  y_{n+1} &=&  y_n + \RY i1m P_i k_i(h),\\
  k_1(h) &=&  h f(x_n,y_n);\\
  k_2(h) &=&  h f\big(x_n+ \alpha_2 h, y_n + \beta_{21} k_1(h)\big);\\
  \vdots\\
  k_m(h) &=&  h f\big(x_n + \alpha_m h,y_n + \beta_{m1}k_1(h) + \dots + \beta_{m,m-1} k_{m-1}(h)\big).
\end{eqnarray*}

Количество параметров, конечно, ужасает. Но мы же поняли уже, что это квадратурные формулы.

Теперь оценим ошибки.
Мы считаем, что точное значение в $x_n$ мы знаем, то есть $y(x_n) = y_n$. Введём функцию ошибки (локальную погрешность) $\phi(h) = y(x_n+h) - y_{n+1}$. Безусловно $\phi(0) = 0$.

Разложим функцию $\phi$ в тейлоровский ряд, чтобы как можно больше коэффициентов разложения ушли в ноль. То есть потребуем, чтобы $\phi'(0) = \phi''(0) = \dots = \phi^{(s)}(0) = 0\ne\phi^{(s+1)}(0)$. Тогда $\phi(h) = h^{s+1} \phi^{(s+1)}(0) + O(h^{s+2})$.

Примем следующее без доказательства.
\begin{The}
Глобальная погрешность метода $E_h = O(h^s)$.
\end{The}

А как собственно эти методы строить? Занятие очень трудоёмкое. Настолько, что большинство методов носит именной характер. Мы построим для $m=2$.
\subsection{Построение метода Рунге"--~Кутта}
\begin{Def}
Величина $s$ "--- порядок точности метода.
\end{Def}
Пусть сначала $m=1$. Тогда
\[
  y_{n+1} = y_n + p_1 k_1(h),\quad k_1(h) = h f(x_n,y_n).
\]
Давайте я, пока мы будем выводить методы, буду писать вместо $x_n, y_n, y(x_{n+1})$ соответственно $x,y,y(x+h)$. 
\[
  \phi(h) = y(x+h) - y(x) - p_1 h f(x,y).
\]
Радостно видим, что $\phi(0) = 0$. Теперь надо потребовать, чтобы $\phi'(0) = 0$. Это мы хотим потребовать. Давайте выпишем вообще $\phi'$. Что это вообще такое будет.
\[
  \phi'(h) = y'(x+h) - p_1f(x,y),\quad y' = f.
\]
то есть $\phi'(0) = f(x,y)(1-p_1)$. То есть при $m=1$, если мы хотим, чтобы производная была равна нулю, то надо требовать $p_1=1$. Вполне естесственно. Дальше можно, конечно, поэкспериментировать  и убедиться, что вторая производная не ноль. Соответственно, метод первого порядка. 
\subsubsection{При $m=2$}
Теперь посмотрим, что будет, если $m=2$. Всё будет уже впечатляюще.
\begin{eqnarray*}
 y_{n+1} &=&  y_n + p_1 k_1(h) + p_2 k_2(h);\\
 k_1(h) &=&  h f(x_n,y_n);\\
 k_2(h) &=& hf\big(x_n + \alpha_2h,y_n + \beta_{21} k_1(h)\big).
\end{eqnarray*}
Как выглядит функция ошибки
\[
  \phi(h) = y(x+h) - y - p_1 h f(x,y) - p_2 h f\big(x + \alpha_2 h, y+ \beta_{21} h f(x,y)\big).
\]
Давайте обозначим $\ol x = x + \alpha_2 h$, $\ol y = y + \beta_{21} h f(x,y)$.
\begin{eqnarray*}
  \phi'(h) &=&  y'(x+h) - p_1 f(x,y) - p_2 f(\ol x,\ol y) - p_2 h
 \big[ \alpha_2 f'_x(\ol x,\ol y) + \beta_{21} f'_y(\ol x,\ol y) f(x,y)\big];\\
  \phi''(h) &=&  y''(x+h) - 2 p_2 \big(\alpha_2 f_x(\ol x,\ol y) + \beta_{21} f'_y(\ol x,\ol y) f(x,y)\big) -\\
	&-& p_2 h\big( \alpha_2^2 f''_{xx} (\ol x,\ol y) + 2 \alpha_2 \beta_{21} f''_{xy}(\ol x,\ol y)f(\ol x,\ol y) + \beta_{21}^2 f''_{yy}(\ol x,\ol y) f^2(\ol x,\ol y).
\end{eqnarray*}

Давайте вы мне поверите на слово, что третью производную не надо выписывать. Всё равно её сделать нулём не удастся.
\begin{eqnarray*}
\phi'(0)&=& (1-p_1 - p_2) f(x,y);\\
\phi''(0) &=& (1 - 2p_2\alpha_2)f'_x(x,y) + (1 - 2p_2\beta_{21})f'_y(x,y)f(x,y).
\end{eqnarray*}

Порадуемся некоторое время. Мы хотим, чтобы в ноль обратились производные при достаточно широком классе правых частей. Итак у нас получается три уравнения на коэффициенты, вообще говоря, не линейных.
\[
\begin{cases}
 1 - p_1 - p_2 = 0\\
 1 - 2 p_2 \alpha_2 = 0\\
 1 - 2 p_2 \alpha_{b1} = 0.
\end{cases}
\]
Если мы сможем эту систему решить, то получим, что $\phi(h) = \frac{h^3}6 \phi'''(0) + O(h^4)$. Из системы мы неизбежно получаем $\beta_{21} = \alpha_2$. Решений много. Завирсируем, допусим $\alpha_2$. Тогда $\beta_{21} = \alpha_2$, $ p_2 = \frac1{2\alpha_2}$, $p_1 = 1-p_2$.

Итак, мы получаем метод с глобальной погрешностью $h^2$.

Если положить, $\alpha_2=1$, мы получим вот такой метод
\[
  \begin{cases}
  y_{n+1} = y_n + \frac12(k_1 + k_2);\\
  k_1 = h f(x_n,y_n);\\
  k_2 = h f(x_n + h, y_n + k_1).
  \end{cases}
\]
Ещё симпатичная формула получается для $\alpha= \frac12$.
\subsubsection{Большие порядки}
Что будет в этом случае. Мы рассмотрели $m=1$, получили $s=1$, рассмотрели $m=2$, получили $s=2$. Примем без доказательства примем, что для $m=3$ получим $s=3$.
\[
  \begin{cases}
  y_{n+1} = y_n + \frac16 (k_1 + 4k_2+ k_3);\\
  k_1 = h f(x_n,y_n);\\
  k_2 = h f\left( x_n + \frac h2,y_n + \frac{k1}2  \right);\\
  k_3 = hf(x_n + h,y_n - k_1 + 2 k_2)
\end{cases}
\]
Обратите внимание, что коэффициенты могут быть и отрицательны.

Для $m=4$ получится $s=4$. Это чаще всего называется методом Рунге. 
\[
\begin{cases}
  y_{n+1} = y_n + \frac16 (k+1 + 2k_2 + 2k_3 + k_4);\\
  k_1 = h f(x_n,y_n);\\
  k_2 = h f\left( x_n + \frac h2,y_n + \frac{k_1}2 \right);\\
  k_3 = h f\left( x_n + \frac h2,y_n + \frac{k_2}2 \right);\\
  k_4 = h f\left( x_n + h, y_n + k_3 \right).
\end{cases}
\]

А дальше получается, что при $m=5$ снова удаётся построить только метод $s=4$. И дальше такой явной прямой зависимости между $m$ и $s$ не наблюдается.

\subsection{Правило Рунге}
Мы уже два раза при вычисления, производных и интегрировании, использовали правила Рунге для оценки погрешности. Попробуем и сюда тоже применить правило Рунге.

Предположим, что мы зафиксировали $m$ и построили метод Рунге"--~Кутты с порядком $s$.
\[
  y_{n+1} + y_n + \RY i1m p_i k_i(h),
\]
где $k_i$ как-то вычисляются. Локальная погрешность
\[
  \phi(h) = \frac{\phi^{(s+1)}(0)h^{s+1}}{(s+1)!} + O(h^{s+2}).
\]

Предлагается метод контроля над локальной погрешностью на шаге. Пусть у нас есть $\varepsilon>0$. Сначала мы делаем шаг $h$, получили $y^{(1)} = y_n + \RY i1m p_i k_i(h)$. Теперь сделаем два шага по $h/2$, снова приходим в ту же точку, но со значением $y^{(2)}$.

Мы полагаем
\[
  y^{(1)} - y(x_n+h)\approx c h^{s+1}.
\]
А вторым методом
\[
  y^{(2)} - y(x_n+h)\approx 2c \left( \frac{h}{2} \right)^{s+1}.
\]
Эта $c$ для нас вещь неизвестная. Это какая-то производная правой части уравнения. Но вот эти $y^{(1)}$, $y^{(2)}$ у нас есть. Вычитаем выражения, одно из другого.
\[
  c h^{s+1}\approx \frac{ y^{(1)} - y^{(2)}}{1 - 2^{-s}} = \rho(h).
\]
Далее мы сравнимаем $\rho(h)$ с $\varepsilon$. Если $\rho$ меньше, считаем следующуу точку; если больше, уменьшаем шаг.
