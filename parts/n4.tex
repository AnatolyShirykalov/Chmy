\subsection{Ошибки в начальных данных}
До сих пор, пока мы строили численные методы решения дифференциальных уравнений, мы не учитывали ошибок округления при арифметике. Мы считали, что все ошибки из того, что дифференциальное уравнение заменяем разностным.

В реальности надо учитывать округление. Рассмотрим дифференциальную задачу
\begin{equation*}
\begin{cases}
y'(x) = f\big(x,y(x)\big);\\
y(x_0) = y_0.
\end{cases}
\end{equation*}
Вопрос учёта всех ошибок округления достаточно сложный. Разберём простейшую ситуацию: мы ошибаемся только в начальных условиях. И даже тут мы столкнёмся с серьёзными проблемами.

Я возьму специфическую задачу.
\begin{equation*}
\begin{cases}
y' = \lambda y;\\
y(0) = y_0.
\end{cases}
\end{equation*}
Чему может равняться $\lambda$? У уравнения есть решение $y(x) = y_0 e^{\lambda x}$. Добавим дрожащей рукой возмущение
\begin{equation*}
\begin{cases}
\Til y' = \lambda \Til y;\\
Til y(0) = y_0 + \varepsilon.
\end{cases}
\end{equation*}

Нам нужны $\lambda<0$, иначе даже точные решения будут сильно отличаться при малых $\e$. Нам нужна устойчивая дифференциальная задача.

Если же $\lambda<0$, исходная задача хорошая. Мы вправе требовать, чтобы наши численные методы были устойчивыми к возмущениям начальных данных.

Строгое определение устойчивости у нас появится только через пару лекций. Пока буду давать определения не совсем строгие.

Будем применять для нашей задачи метод Эйлера.
\[
  \begin{cases}
\frac{ y_{n+1} - y_n}{h} = f(x_n,y_n);\\
y_0.
\end{cases}
\]
То есть расчётная формула $y_{n+1} = y_n + h \lambda y_n$, $y_0$.

На ряду с этой задачей рассмотрим возмущённую $\Til y_{n+1} = \Til y_n + h\lambda \Til y_n$, $\Til y_0 = y_0 + \e$. При этом у нас строго $\lambda<0$.

Что из себя представляет разность
\[
  \Til y_n - y_n = (1 + h\lambda)^n(y_0+\e) - (1+ h\lambda)^n y_0 = (1 + h\lambda)^n\e.
\]
То есть для того, чтобы ошибка не росла, нам надо, чтобы $|1+h\lambda|\le 1$, отсюда $h\le \frac{2}{|\lambda|}$. Если ни о чём особо не задумываться, то ну ок, пусть такой шаг и будет. Но предположим, что у нас $|\lambda|\gg1$, $\lambda<0$. Тогда шаг будет очень маленький и до единицы мы будем проходить за очень даже ощутимое время.

\subsubsection{Двумерный случай}
Теперь попытаемся перейти к двумерной задаче. Проблему я обозначил, теперь увидим, как она приведёт нас в затруднительной положение.
\begin{equation*}
\begin{cases}
y'_1(x) = f_1(x,y_1,y_2);\\
y_2'(x) = f_2(x,y_1,y_2);\\
y_1(0) = y_0^1;\\
y_2(0) = y_0^2.
\end{cases}
\end{equation*}

Пока не пробовали, но на самом деле решать умеем.
\begin{eqnarray*}
\frac{y_{n+1}^1 - y_n^1}{h} &=&  f_1(x_n,y^1_n,y_n^2);\\
\frac{y_{n+1}^2 - y_n^2}{h} &=&  f_2(x_n,y^1_n,y_n^2);\\
\end{eqnarray*}
Можно повторить усилия, получить, что локальная ошибка порядка $h^2$, глобальная "--- порядка $h$.

С ошибками в начальных данных у нас в одномерном случае возникло ограничение на шаг. Нужен достаточно малый шаг, чтобы задача была устойчивой. Такие задачи называют жёсткими. При этом на практике можно выделять отрезки, на которых знаем, что точное решение почти ноль и нечего его там считать.

Давайте какой-нибудь пример приведу.

Пусть $\ve y = (y_1,y_2)^T$, $\ve y'(x) = A y(x)$, $\ve y(0) = \begin{pmatrix}
1 & 1
\end{pmatrix}^T$,
\[
  A = \begin{pmatrix}
998 & 1998 \\
-999 & -1999
\end{pmatrix}
\]
Ой, этот пример не очень удачный.

Пусть всё-таки $\ve y(0) = \begin{pmatrix}
 2 & 1
\end{pmatrix}^T$,
\[
  A = \begin{pmatrix}
  -2 & -998\\
  0 & -1000
\end{pmatrix}
\]
Решение имеет вид
\begin{eqnarray*}
 y_1(x) &=&  e^{-2x} + e^{-1000x};\\
  y_2(x) &=&  e^{-1000x}.
\end{eqnarray*}
Чтобы считать $y_2$, нам нужен шаг $h\le \frac2{1\,000}$.
Но выкинуть отрезок мы не можем, потому что есть ещё $y_1$.

\subsubsection{Методом Рунге"--~Кутта}
У нас появилось понятия жёсткой задачи, когда есть условие на шаг для того, чтобы задача была устойчивой. Попытаемся взять что-то более сильное, чем метод Эйлера. Сможем ли хотя бы на порядок глобальной ошибки метода снизить ограничение на шаг, то будет хорошо.

Применияем
\begin{eqnarray*}
y_{n+1} &=&  y_n + \frac12 (k_1 + k_2);\\
k_1 &=&  h f(x_n,y_n);\\
k_2 = h f(x_n + h, y_n + h).
\end{eqnarray*}
Решаем простейшую задачу $y' = \lambda y$, $\lambda<0$, $y(0) = y_0$, $|\lambda|\gg 1$. И возмутим $\Til y(0) = y_0 + \e$.
\[
  y_n = y_{n-1} + \frac12 \big( \lambda h y_{n-1} + \lambda h (y_{n-1} + \lambda h y_{n-1})\big) = 
\left( 1 + \lambda h + \frac{\lambda^2 h^2}2 \right) y_{n=1} = \left( 1 + \lambda h + \frac{\lambda^2 h^2}2\right)^n y_0.
\]
Что в общем логично, получили кусок разложения $e^{\lambda h}$ в ряд. Разность имеет вид
\[
 \Til y_n - y_n = \left( 1 + \lambda h + \frac{\lambda^2 h^2}2\right)^n\e.
\]
Чтобы была устойчивость, нужно $\left| 1 + \lambda h + \frac{\lambda^2 h^2}2 \right|\le 1$. Если решить, получим удручающий результат  $h\le \frac2{|h|}$. Ничего не изменилось.

Если взять классический метод Рунге с погрешностью порядка $h^4$.
\begin{eqnarray*}
y_{n+1} &=&  y_n + \frac16(k_1 + 2 k_2 + 2k_3 + k_4);\\
k_1 &=& h f(x_n,y_n);\\
k_2 &=&  h f\left( x_n + \frac h2,y_n + \frac{k_1}2 \right);\\
k_3 &=& h f\left( x_n + \frac h2,y_n + \frac{k_2}2 \right);\\
k_4 &=& h f(x_n + h, y_n + k_3).
\end{eqnarray*}
Тут если поковыряться, получим
\[
\left| 1 + \lambda h + \frac{h^2\lambda^2}2 + \frac{\lambda^3h^3}6 + \frac{h^4h^4}{24} \right|\le 1.
\]
Условие в итоге $h < \frac{2{,}785}{|\lambda|}$.

Надо понять, что такое всё-таки задача жёсткая. Оказывается, ограничение завивит не от метода, а от самой задачи.
\subsection{Устойчивость}
Идея такая. Есть у нас $\ve y'(x) = A(x) \ve y(x)$ система с начальным условием $\ve y(x_0) = \ve y_0$. Решается система на отрезке $x\in[x_0,x_0+X]$. Назовём эту систему жёсткой, если выполнено три условия
\begin{enumerate}
\item $X\max\big|\lambda_i(A)\big|\gg 1$. (Собственные значения зависят от $x$, поэтому максимум берётся и по собственным значениям и по всем $x$.)
\item $X\max \Re\big(\lambda_i(A)\big)\sim 1$.
\item $X\max \Big|\Im\big(\lambda_i(A)\big)\Big|\sim 1$.
\end{enumerate}

А что такое $\gg1$? Смотря в какой ситуации мы находимся. С современным развитием техники $h\sim 10^{-6}$ "--- нормально. Никто наши страдания не поймёт. $h\sim 10^{-8}$, ну уже определённое напряжение возникает. Если $h\sim 10^{-12}$, это уже, конечно, недопустимо.

Хороший был бы семестровый курс на методы борьбы с жёсткими системами. Мы просто посмотрим, в какие стороны можно идти.
\subsubsection{Метод решения проблемы на примере самой простой задачи}
Давайте попробуем сдвинуться с другого пути. По-прежнему $y' = f(x,y)$, $y(0) = y_0$, $x\in[0,X]$.

Как был построен метод Эйлера? Взяли и заменили производную разностью вперёд, а могли взять разность назад. Никто нам не мешал написать
\[
  \frac{y(x+h) - y(x)}{h} = f\big(x+h,y(x+h)\big) + O(h).
\]
К какой разностной схеме это приведёт?
\[
  \frac{y_{n+1} - y_n}{h}f(x_{n+1},y_{n+1}).
\]
Как вообще отсюда $y_{n+1}$ получить? Чтобы об этом думать, надо сначала понять, а зачем это вообще нужно.

Это называется неявный метод Эйлера. Погрешность у него будет такая же, как у прямого.

Рассмотрим ту же задачу $y' = \lambda y$, $\lambda <0$, $y(0) = y_0$. Тогда
\[
  y_n = y_{n-1} + h \lambda y_n.
\]
Выражаем отсюда $y_n$ (волноваться при $\lambda<0$ не приходится, на ноль не делим)
\[
  y_n = \frac{y_{n_1}}{1- h\lambda} = \dots = \frac{y_0}{(1-h\lambda)^n}.
\]
А разность имеет вид
\[
  |\Til y_n - y_n| = \left|\frac\e{(1-h\lambda)^n} \right| < |\e|.
\]
Получили метод, который для нашей задачи просто устойчивый.

Можно ли сформулировать класс задач с такой же проблемой. Среди задач, которые не являются линейными, например. Задача
\[
  f'_y<0;\quad |f'_y|\gg1.
\]
Для такой задаче неявный методй Эйлера уже не очень хорошо. Как выуживать $y_{n+1}$? Эту проблему мы унесём на следующую лекцию.
\subsubsection{Альтернативный подход решения проблемы неустойчивости}
Даже явный метод Эйлера можно переделать так, чтобы метод стал устойчивым. Метод именной, называется методом Лебедева. Мы доказывать ничего не будем, доказательство там не тривиальное.

Рассмотрим задачу
\[
  y' + M y = 0,\quad M\gg 1,\quad y(x_0) = y_0.
\]
Хотим использовать явный метод Эйлера
\[
  \frac{y_{n+1} - y_n}h + M y_n = 0.
\]
Нам известно, что тогда для устойчивости нужно $h\le\frac 2M$.

Пусть мы можем сделать только  $N$ шагов, тогда мы можем только ушагать до $x_0 + \frac {2N}M$. Но давайте попробуем начать менять шаг. Хуже не будет.
\[
  \frac{y_{n+1} - y_n}{h_{n+1}} + M y_n = 0.
\]
Тогда ограничение на шаги имеет вид
\[
  \big|(1 - h_1M)(1-h_2M)\dots(1-h_NM)\big|\le 1.
\]
При этом ограничении решим задачу $\RY i1N h_i\to \max$.

Оказывается, что прошагаем мы до $x_0 + \frac{2(2N)^2}M$. Это существенное улучшение.

Всё, конечно, гораздо тяжелее. Если система, нужны гарантии, чтобы не было дополнительных причин для роста погрешности. Просто обратите внимание, что бывает взгляд и с совсем другой стороны.
