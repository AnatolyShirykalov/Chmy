\section{Численные методы алгебры}
Приступаем к последней большой теме этого семестра. Надо сказать, что не всё, что обычно входит в курс, будет рассказано. Что нас будет интересовать.
\begin{enumerate}
\item $Ax = b$, матрица $A$ размерности $n\times n$, $\exists\ A^{-1}$. нас интересует вектор $x$. Все попутные вещи, обратная матрица или, не дай боже, определитель, здесь не вычисляются.
\item $Ax = b$, $A (m\times n)$, $m>n$. Это хорошая ситуация. Здесь запросто решения может не существовать. Здесь надо на самом деле договориться, что мы будем называть как-бы решением, а затем придумать, как мы будем это решение находить.
\end{enumerate}
Есть ещё класс вопросов связанных с нахождением собственных значений. Не дай бог все. Обычно наибольшее и наименьшее по модулю. Методы будут непрямые.

Есть ещё часть, называемая итерационными методами. Какое-то время назад все проблемы вычислительной математики упирались в то, что времени нехватало. Сейчас со временем всё хорошо относительно, а с памятью уже появились проблемы. Предположим матрица обладает свойством, что элементы расставлены по какому-то закону и закон известен (например, на каждой диагонали одинаковые элементы), можно хранить меньше данных, но не все действия с этой матрицей можно будет выполнять. Всё что будет можно делать, это умножать её на вектор.

Мы будем обращать внимание на то, что мы находимся не в реальной арифметике, а в машинной.

\subsection{Метод Гаусса}
Он не просто так. Относится к методам на основе $LU$-разложения, то есть $A = LU$. Здесь $L$ "--- нижнетреугольная матрица, $U$ "--- верхнетреугольная матрица\footnote{Вообще все методы будут основаны на разложениях $A=LU$ и $A = QR$ "--- ортогональная $Q^TQ = E$ и право-треугольная.}.

Вообще, что можно сказать про элементы главной диагонали $L$ и $U$. Там нет нулей, иначе $A$ вырождена, а это нехорошо.

В матрицах $L$ и $U$ неизвестных нам элементов $n^2+n$. Число уравнений на них $n^2$. То есть $n$ явно лишних. И мы можем в это разложение можем впихнуть $A = L D D^{-1} U$, диагональную матрицу $D$. Таким образом можем управлять диагональными элементами $L$ или $U$. Если мы добиваемся $u_{ii}=1$, это метод Гаусса. Если $l_{ii} = 1$, это метод Краута.

Это первое, что надо было сказать.

Дальше. Почему это разложение $LU$ существует. 

Введём какой-нибудь формализм. Что из себя представляет метод Гаусса. Мы берём матрицу $A$ и её во что-то торжественно превращаем. Обозначим через $A^{(0)} = (A\mid b)$ расширенную матрицу системы.
\[
  A^{(0)} \to A^{(1)} \to A^{(2)}\to \dots
\]
Переход от матрицы $A^{(i-1)}$ к матрице $A^{(i)}$ состоит вот в чём. Сначала домножаем на $c_i$ ($c_i A^{(i-1)}$)
\[
  c_i = \begin{pmatrix}
  1 \\
  & \ddots \\
  & & 1\\
  & & & (a_{ii}^{(i-1)})^{-1}\\
  & & & & 1\\
  & & & & & \ddots\\
  & & & & & & 1
\end{pmatrix}
\]
Дальше умножаем ещё на $c_i'$, где
\[
  c_i = \begin{pmatrix}
  1 \\
  & \ddots \\
  & & 1\\
  & & & a_{ii}^{(i-1)}\\
  & & & \vdots & 1\\
  & & & \vdots & & \ddots\\
  & & &  a_{i,n}^{i-1}& & & 1
\end{pmatrix}
\]

Итак у нас $LU x = L b$. Или же $Lx = Lb$. Таким образом сначала решаем $Lx = Lb$, затем $Ux = b$, это всё $O(n^2)$. Посчитать $LUx = Lb$ есть $O(n^3)$.

Что делать, если в первой строке на первом месте ноль. В первом столбце выбираем самое большое число и ставим на первое место. Далее на $k$ столбце ищем самое большое число среди оставшихся строк ($j\ge k$) и переставляем с наибольшим элементом.

Пусть $P_{ij}$ "--- элементарная матрица. При умножении слева её, меняются строки местами. А если её умножить справа, то столбцы меняются местами.
Если нам хочется поменять местами строки, появляется
\[
  P_{ij} A x = P_{ij}b
\]
Матрица меняется, правая часть меняется, но иксы остаются на месте.

Если менять местами столбцы, то дело хуже обстоит
\[
  A P_{ij} P^{-1}_{ij} x =  b.
\]
Поменяли столбцы в $A$ и поменялись иксы. Про это часто забывают, нужно представлять, от чего это происходит.

Исторически этот алгоритм на первом месте. Но считается не таким устойчивым, как основанные на разложении $QR$.

\subsection{Метод Холевского}
Пусть у нас матрица симметричная и положительно определённая. Тогда её можно трансформировать методом Холевского. 

Матрица называется симметричной, если $A^T = A$. Положительно определённой, если $\forall x\ne 0\pau (Ax,x)>0$. Записывается это вот так $A  = A^T >0$. Она автоматически является невырожденной. Действительно пусть $Ax = 0$ и $x\ne 0$, то ведь $(Ax,x)=0$ при ненулевом иксе, что невозможно.

\begin{Ut}
Если у нас $A = A^T>0$, то $a_{ii}>0$.
\end{Ut}
Это легко понять, потому что $a_{ii} = (Ae_i,e_i)>0$.

Это хорошо, потому что здесь можно не переставлять строки, деления на ноль не произойдёт на автомате.

Отсюда возникает идея метода Холевского. Здесь $LU$-разложение из себя что представляет: $A = LL^T$. Здесь число уравнений совпадает с числом неизвестных. Кладём $l_{11} = \sqrt{a_{11}}$, первый столбец сооружаем $a_{i1} = \frac{a_{i1}}{l_11}$, $i>2$. Далее
\[
  l_{ii} = \bigg(a_{ii} - \RY j1{i-1} l^2_{ij}\bigg)^{\frac12},\pau i=2,\dots,n.
\]
Далее
\[
  l_{ij} = \frac1{l_{jj}} \bigg( a_{ij} - \RY k1{i-1} l_{jk}l_{ik}\bigg),\pau j=2,\dots,n, i = j+1,\dots,n.
\]
Ну и наконец $l_{ij}=0$ для $i<j$.

Теперь перейдём к альтернативному подходу. Не буду оговаривать, что определение касается только вещественного случая.
\begin{Def}
  Матрица $Q$ называется ортогональной, если $QQ^T = E$.
\end{Def}
Можно ряд свойств определить
\begin{enumerate}
\item $Q^{-1} = Q^T$;
\item $\det Q = \pm 1$;
\item $Q^T$ ортогональна;
\item Произведение ортогональных ортогонально.
\end{enumerate}

Пусть решаем $Ax=b$. Пусть мы можем представить $A = QR$. То есть наша система равносильна такой $QRx = b$. Дальше $Rx = Q^T b$, её быстро решаем. На самом деле всё будет не совсем так происзодить, сейчас разберёмся.

\subsubsection{Метод вращений}
Первый подход $QR$-разложения. Сначала определим матрицу элементарного вращения $T_{ij}(\phi)$, определяется номерами строк, на которые она воздействует, и угол $\phi$.
\[
 c_i = \begin{pmatrix}
  1 \\
  & \ddots \\
  & & 1\\
  & & & \cos\phi & -\sin\phi\\
  & & & & 1\\
  & & & & & \ddots\\
  & & & & & & \sin\phi & \cos\phi
\end{pmatrix}
\]
Допишу потом, понятно, что тут. Тут $t_{ii} = t_{jj} = \cos\phi$, $t_{ij} = -t{ji} = \sin\phi$, $t_{kk}=1$.

Начинаем вращать $A^{(1)} = T_{ij}(\phi)A$. Получаем
\[
  a_{ik}^{(1)} = a_{ik} \cos\phi - a_{jk} \sin\phi;\qquad
  a_{jk}^{(1)} = a_{ik}\sin\phi + a_{jk} \cos\phi.
\]

Хотим, чтобы какой-то $a_{jk}^{(1)} = 0$, для этого
\begin{roItems}
\item $a_{ik}^2 + a_{jk}^2\ne 0$. Тогда $\cos\phi = \frac{ a_{ik}}{\sqrt{a_{ik}^2+ a_{jk}^2}}$, $\sin\phi = \frac{-a_{jk}}{\sqrt{a_{ik}^2 + a_{jk}^2}}$;
\item Если же $a_{ik}^2 + a_{jk}^2 = 0$, $\cos\phi = 1, \sin \phi = 0$, то есть ничего не делаем.
\end{roItems}

Вот у нас есть матрица $A$. Мы хотим, чтобы в первом столбце первый элемент был каким-то числом, остальные нули. Делаю $T_{12}(\phi_2) A$ так, чтобы $a_{21}^{(1)} = 0$. И дальше также. На первом месте явно не ноль, потому что тогда в этом равенстве будет только одна вырожденная матрица, чего не может быть.
\[
  \underbrace{T_{1n}(\phi_n)\dots T_{13}(\phi_3)T_{12}(\phi_2)}_{T_1} A = 
\begin{pmatrix}
  a_{11}^{(1)} & \dots\\
  0 & \dots\\
  \vdots
\end{pmatrix}
\]
Обозначаю $A^{(1)} = T_1 A$. Дальше зануляю как обычно следующий столбец
\[
  T_{2n}\dots T_{23} A^{(1)}.
\]
Обозначаю всё же $T_2$. И так далее $T^T = T_n\dots T_1$, Выходит $Ax = b$ переходит в $T^TRx = b$ или же $Rx = Tb$.

Когда будем вычислять углы, может быть деление на маленькие знаменатели. Но есть литература, в которой показывается, что здесь устойчивость больше, чем в методе Гаусса.
Здесь тот же самый $O(n^3)$ как и в методе Гаусса.

\subsubsection{Метод Отражений}
Можно было и одним ограничиться. Но не каждый раздел высшей математики может похвастаться, то может быть объяснён на пальцах, но вещь серьёзная.

Метод отражений стоит особняком. Тут сразу за один проход будем весь столбец получать.

Мы работаем в $\R^n$. Рассмотрим единичный вектор $\ol\omega$, то есть $\|\ol\omega\|_2 = \sqrt{(\ol\omega,\ol\omega)} = 1$. Составим такую матрицу
\[
  U = E - 2\ol\omega \ol\omega^T.
\]
Это всё есть матрица с очень быстро вычисляемыми элементами. Какими полезными свойствами она обладает.
\begin{enumerate}
\item $U = U^T$.

Как мы траспонируем сумму $(A+B)^T = A^T + B^T$. А если произведение $(AB)^T = B^TA^T$. Вот тут и всё.
\item $UU^T = E$.

Оставлю в качестве домашнего задания. Очень полезное упражнение.
\end{enumerate}
А что будет, если мы применим нашу матрицу $U$ к какому-то вектору $z$. Вектор $z$ мы можем разбиать на $z= z_1+z_2$, причём $z_1\perp \ol\omega$, ну а $z_2\parallel \ol\omega$.
Оказывается, что в результате получится
\[
  Uz = z_1 -  z_2.
\]
Отражение. Ортогональная составляющая без изменения, а параллельная меняется. Как это удобно показать
\[
  Uz_2 = z_2 - 2(z_2,\omega)\omega = z_2-2z_2 = -z_2.
\]

А теперь поставим такую задачу. Пусть имеются два вектора: $\ol S$ и $\ol e$, причём $(\ol e,\ol e)=1$. Вопрос: подобрать такой вектор $\omega$, чтобы матрица отражения, построенная на этом векторе $\omega$ давала бы
\[
 U\ol S = \alpha \ol e.
\]

Ну на самом деле решение здесь достаточно простое. Нужно, чтобы $U\ol S = \alpha \ol e$. Какая может быть $\alpha$, ну например (можно и с минусом) $\alpha = \sqrt{(S,S)}$. Положим
\[
  \ol \omega = \frac1\rho (\ol S - \alpha \ol e),
\]
где $\rho\colon \|\omega\|_2 = 1$. Проверите, что этот вектор подходит дома. Если один раз проделать руками, то дальше уже не забудете.

Как теперь это применить к решению системы. Пусть есть система $Ax = b$, где $A$ невырождена. В качестве вектора $\ol S$ возьмём первый столбец
\[
  \ol S = \begin{pmatrix}
a_{11}\\
\vdots \\
a_{n1}
\end{pmatrix}
\]
В качестве вектора $\ol e^T = (1,0,\dots,0)$. Отсюда строим $U_1$. Домножаем слева на уравнение (обратите внимание, что матрица умножается дёшево)
\[
  U_1 A x = U_1 b.
\]
На месте первого столбца $\|a_1\|$ сосредоточена на первом же элементе получившейся матрицы, остальные в первом столбце элементы нули.

Теперь начинаем работать в $\R^{n-1}$. В качестве $\ol S^T = (a_{22}^{(1)},\dots, a_{n2}^{(1)})$, $\ol e^T =(1,0,\dots,0)\in \R^{n-1}$. Далее
\[
  \begin{pmatrix}
1 &0\\
1 & U_2
\end{pmatrix}U_1 A x = \begin{pmatrix}
1 &0\\
1 & U_2
\end{pmatrix} U1 b.
\]

Снова затраты $O(n^3)$, а при обратном ходе затраты $O(n^2)$. Последние два подхода предпочтительней, чем метод Гаусса. Есть такие матрицы, которые методом Гаусса не обрабатываются, а этими получается нормально. Есть конечно такие, где всё плохо. Есть такая матрица Гильберта
\[
  G_{ij} = \frac1{i+j-1}.
\]
До $n=6$ ещё будет работать, дальше система сломается. Матрица отвратительно обусловлена. Причём алгоритм не кричит, что делит на ноль, он всё старательно считает. Но если потом результат подставить в систему, ничего похожего на правую часть вы не получите.
